---
id: monitoring
title: Monitoring
description: Comprehensive monitoring guide for Neuralock server operations
sidebar_position: 4
---

import { Card, CardHeader, CardTitle } from "@site/src/components/elements";
import { Callout } from "@site/src/components/elements";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { FileCollapsibleCodeBlock } from "@site/src/components/elements/CodeBlock";
import { MermaidDiagram } from '@site/src/components/elements';
import MonitoringArchitecture from '!!raw-loader!./assets/monitoring-architecture.mermaid';
import PrometheusConfig from '!!raw-loader!./assets/prometheus.yml';
import GrafanaDashboard from '!!raw-loader!./assets/grafana-dashboard.json';
import AlertRules from '!!raw-loader!./assets/alert-rules.yml';
import HealthCheckScript from '!!raw-loader!./assets/health-check.py';

# Monitoring

This guide covers comprehensive monitoring strategies for Neuralock server deployments, including metrics collection, alerting, and troubleshooting.

## Monitoring Architecture

<MermaidDiagram content={MonitoringArchitecture} filename="monitoring-architecture.mermaid" />

## Key Metrics

<div style={{ display: 'grid', gridTemplateColumns: 'repeat(auto-fit, minmax(300px, 1fr))', gap: '1rem', marginBottom: '2rem' }}>
  <Card>
    <CardHeader>
      <CardTitle>ðŸš€ Performance Metrics</CardTitle>
    </CardHeader>
    <ul>
      <li>Request latency (p50, p95, p99)</li>
      <li>Throughput (requests/second)</li>
      <li>Active connections</li>
      <li>CPU and memory usage</li>
      <li>Disk I/O operations</li>
    </ul>
  </Card>
  
  <Card>
    <CardHeader>
      <CardTitle>ðŸ“Š Business Metrics</CardTitle>
    </CardHeader>
    <ul>
      <li>Active sessions count</li>
      <li>Shares stored/retrieved</li>
      <li>Threshold operations</li>
      <li>Contract validations</li>
      <li>Permission checks</li>
    </ul>
  </Card>
  
  <Card>
    <CardHeader>
      <CardTitle>ðŸ”’ Security Metrics</CardTitle>
    </CardHeader>
    <ul>
      <li>Failed authentication attempts</li>
      <li>Rate limit violations</li>
      <li>Invalid signatures</li>
      <li>Blockchain errors</li>
      <li>Encryption failures</li>
    </ul>
  </Card>
</div>

## Monitoring Stack Setup

<Tabs defaultValue="prometheus">
  <TabItem value="prometheus">
    ### Prometheus Setup

    Prometheus collects and stores time-series metrics from the Neuralock server.

    #### 1. Install Prometheus

    ```bash
    # Docker installation
    docker run -d \
      --name prometheus \
      -p 9090:9090 \
      -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \
      -v prometheus_data:/prometheus \
      prom/prometheus

    # Native installation
    wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
    tar xvf prometheus-*.tar.gz
    cd prometheus-*
    ./prometheus --config.file=prometheus.yml
    ```

    #### 2. Configure Prometheus

    <FileCollapsibleCodeBlock content={PrometheusConfig} filename="prometheus.yml" />

    #### 3. Available Metrics

    **Request Metrics:**
    ```
    # Request rate by endpoint
    rate(neuralock_requests_total[5m])

    # Request duration percentiles
    histogram_quantile(0.95, rate(neuralock_request_duration_seconds_bucket[5m]))

    # Error rate
    rate(neuralock_requests_total{status=~"5.."}[5m])
    ```

    **Session Metrics:**
    ```
    # Active sessions
    neuralock_active_sessions

    # Session creation rate
    rate(neuralock_sessions_created_total[5m])

    # Session expiration rate
    rate(neuralock_sessions_expired_total[5m])
    ```

    **Storage Metrics:**
    ```
    # LevelDB size
    neuralock_storage_bytes{type="leveldb"}

    # Redis memory usage
    neuralock_storage_bytes{type="redis"}

    # Share operations
    rate(neuralock_shares_stored_total[5m])
    rate(neuralock_shares_retrieved_total[5m])
    ```

    <Callout type="info" title="Metric Naming Convention">
      All Neuralock metrics follow the pattern: `neuralock_<component>_<metric>_<unit>`
    </Callout>
  </TabItem>

  <TabItem value="grafana">
    ### Grafana Dashboards

    Grafana provides visualization for Prometheus metrics.

    #### 1. Install Grafana

    ```bash
    # Docker installation
    docker run -d \
      --name grafana \
      -p 3000:3000 \
      -v grafana_data:/var/lib/grafana \
      -e GF_SECURITY_ADMIN_PASSWORD=admin \
      grafana/grafana

    # Access at http://localhost:3000
    # Default login: admin/admin
    ```

    #### 2. Import Dashboard

    <FileCollapsibleCodeBlock content={GrafanaDashboard} filename="grafana-dashboard.json" />

    #### 3. Key Dashboard Panels

    <div style={{ display: 'grid', gridTemplateColumns: 'repeat(auto-fit, minmax(300px, 1fr))', gap: '1rem' }}>
      <Card>
        <CardHeader>
          <CardTitle>API Overview</CardTitle>
        </CardHeader>
        <ul>
          <li>Request rate graph</li>
          <li>Error rate percentage</li>
          <li>Response time heatmap</li>
          <li>Active connections gauge</li>
        </ul>
      </Card>
      
      <Card>
        <CardHeader>
          <CardTitle>Session Management</CardTitle>
        </CardHeader>
        <ul>
          <li>Active sessions counter</li>
          <li>Session creation/expiry rate</li>
          <li>Session duration histogram</li>
          <li>Failed authentications</li>
        </ul>
      </Card>
      
      <Card>
        <CardHeader>
          <CardTitle>Storage Health</CardTitle>
        </CardHeader>
        <ul>
          <li>Database size trends</li>
          <li>Cache hit rate</li>
          <li>Storage operations/sec</li>
          <li>Compaction statistics</li>
        </ul>
      </Card>
    </div>

    #### 4. Custom Queries

    ```promql
    # API Success Rate (SLI)
    (
      sum(rate(neuralock_requests_total{status=~"2.."}[5m]))
      /
      sum(rate(neuralock_requests_total[5m]))
    ) * 100

    # P95 Latency by Endpoint
    histogram_quantile(0.95,
      sum by (endpoint, le) (
        rate(neuralock_request_duration_seconds_bucket[5m])
      )
    )

    # Blockchain RPC Errors
    sum by (error_type) (
      rate(neuralock_blockchain_errors_total[5m])
    )
    ```
  </TabItem>

  <TabItem value="alerting">
    ### Alerting Configuration

    Configure alerts for critical issues and SLA violations.

    #### 1. Alert Rules

    <FileCollapsibleCodeBlock content={AlertRules} filename="alert-rules.yml" />

    #### 2. Alert Severity Levels

    | Severity | Response Time | Examples |
    |----------|---------------|----------|
    | **Critical** | < 5 minutes | Service down, data loss risk |
    | **High** | < 30 minutes | Performance degradation, high error rate |
    | **Medium** | < 2 hours | Elevated latency, capacity warnings |
    | **Low** | < 24 hours | Informational, trends |

    #### 3. Alertmanager Configuration

    ```yaml
    # alertmanager.yml
    global:
      resolve_timeout: 5m
      smtp_from: 'alerts@neuralock.io'
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_auth_username: 'alerts@neuralock.io'
      smtp_auth_password: 'password'

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'team-ops'
      routes:
      - match:
          severity: critical
        receiver: 'team-ops-critical'
        
    receivers:
    - name: 'team-ops'
      email_configs:
      - to: 'ops@neuralock.io'
        
    - name: 'team-ops-critical'
      email_configs:
      - to: 'ops@neuralock.io'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
      slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK'
        channel: '#alerts-critical'
    ```

    #### 4. Alert Response Playbooks

    <Callout type="warning" title="Critical Alert: Service Down">
      1. Check `/health` and `/ready` endpoints
      2. Verify Redis and LevelDB connectivity
      3. Check blockchain RPC status
      4. Review recent deployment changes
      5. Check system resources (CPU, memory, disk)
      6. Escalate if not resolved in 15 minutes
    </Callout>
  </TabItem>

  <TabItem value="logging">
    ### Centralized Logging

    Aggregate logs from all server instances for analysis.

    #### 1. Log Aggregation Stack

    ```yaml
    # docker-compose.yml for ELK stack
    version: '3.8'
    services:
      elasticsearch:
        image: elasticsearch:8.8.0
        environment:
          - discovery.type=single-node
          - xpack.security.enabled=false
        volumes:
          - es_data:/usr/share/elasticsearch/data
        ports:
          - "9200:9200"
          
      logstash:
        image: logstash:8.8.0
        volumes:
          - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
        depends_on:
          - elasticsearch
          
      kibana:
        image: kibana:8.8.0
        ports:
          - "5601:5601"
        environment:
          - ELASTICSEARCH_URL=http://elasticsearch:9200
        depends_on:
          - elasticsearch
    ```

    #### 2. Logstash Configuration

    ```ruby
    # logstash.conf
    input {
      tcp {
        port => 5000
        codec => json
      }
      file {
        path => "/logs/neuralock/*.log"
        start_position => "beginning"
        codec => json
      }
    }

    filter {
      if [level] == "ERROR" {
        mutate {
          add_tag => [ "error" ]
        }
      }
      
      geoip {
        source => "client_ip"
        target => "geoip"
      }
      
      date {
        match => [ "timestamp", "ISO8601" ]
      }
    }

    output {
      elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "neuralock-%{+YYYY.MM.dd}"
      }
      
      if "error" in [tags] {
        email {
          to => "ops@neuralock.io"
          subject => "Neuralock Error: %{message}"
        }
      }
    }
    ```

    #### 3. Structured Logging

    ```python
    # Example structured logging in application
    import structlog

    logger = structlog.get_logger()

    # Log with context
    logger.info("session_created",
        user_id=user_public_key,
        contract=contract_address,
        ttl=ttl,
        server_id=server_name
    )

    # Log errors with full context
    logger.error("share_storage_failed",
        error=str(e),
        object_id=object_id,
        threshold=threshold_config,
        traceback=traceback.format_exc()
    )
    ```

    #### 4. Log Analysis Queries

    ```json
    // Kibana query examples
    
    // Failed authentication attempts
    {
      "query": {
        "bool": {
          "must": [
            { "match": { "event": "authentication_failed" }},
            { "range": { "@timestamp": { "gte": "now-1h" }}}
          ]
        }
      }
    }

    // Slow requests
    {
      "query": {
        "range": {
          "duration_ms": { "gte": 1000 }
        }
      }
    }

    // Error spike detection
    {
      "aggs": {
        "errors_over_time": {
          "date_histogram": {
            "field": "@timestamp",
            "interval": "5m"
          },
          "aggs": {
            "error_count": {
              "filter": { "term": { "level": "ERROR" }}
            }
          }
        }
      }
    }
    ```
  </TabItem>
</Tabs>

## Health Monitoring

### Health Check Implementation

<FileCollapsibleCodeBlock content={HealthCheckScript} filename="health-check.py" />

### Health Check Endpoints

<Tabs defaultValue="basic">
  <TabItem value="basic">
    #### GET /health

    Simple health check for load balancers:

    ```bash
    # Healthy response
    curl http://localhost:8000/api/v1/health
    {
      "status": "healthy",
      "timestamp": "2024-01-01T12:00:00Z"
    }

    # Unhealthy response (503)
    {
      "status": "unhealthy",
      "timestamp": "2024-01-01T12:00:00Z"
    }
    ```

    **Health Criteria:**
    - Service is running
    - Can accept connections
    - No critical errors in last 60 seconds
  </TabItem>

  <TabItem value="detailed">
    #### GET /ready

    Detailed readiness check with dependencies:

    ```json
    {
      "ready": true,
      "checks": {
        "redis": {
          "status": "ok",
          "latency_ms": 2,
          "info": {
            "version": "7.0.11",
            "connected_clients": 5,
            "used_memory": "12MB"
          }
        },
        "leveldb": {
          "status": "ok",
          "size_bytes": 1048576,
          "info": {
            "num_files": 42,
            "compaction_pending": false
          }
        },
        "blockchain": {
          "status": "ok",
          "block_number": 18900000,
          "synced": true,
          "latency_ms": 145
        },
        "disk_space": {
          "status": "ok",
          "free_bytes": 107374182400,
          "used_percent": 45
        }
      },
      "timestamp": "2024-01-01T12:00:00Z"
    }
    ```
  </TabItem>

  <TabItem value="custom">
    #### Custom Health Checks

    ```python
    # Add custom health checks
    @app.get("/api/v1/health/crypto")
    async def crypto_health():
        """Check cryptographic operations health"""
        try:
            # Test key generation
            start = time.time()
            test_key = generate_ephemeral_key()
            keygen_time = time.time() - start
            
            # Test encryption/decryption
            start = time.time()
            test_data = b"test data"
            encrypted = encrypt_data(test_data, test_key)
            decrypted = decrypt_data(encrypted, test_key)
            crypto_time = time.time() - start
            
            return {
                "status": "healthy",
                "checks": {
                    "key_generation": {
                        "status": "ok",
                        "time_ms": keygen_time * 1000
                    },
                    "encryption": {
                        "status": "ok",
                        "time_ms": crypto_time * 1000,
                        "test_passed": decrypted == test_data
                    }
                }
            }
        except Exception as e:
            return JSONResponse(
                status_code=503,
                content={"status": "unhealthy", "error": str(e)}
            )
    ```
  </TabItem>
</Tabs>

## Performance Monitoring

### Key Performance Indicators (KPIs)

<div style={{ display: 'grid', gridTemplateColumns: 'repeat(auto-fit, minmax(300px, 1fr))', gap: '1rem' }}>
  <Card>
    <CardHeader>
      <CardTitle>ðŸŽ¯ Availability SLOs</CardTitle>
    </CardHeader>
    <ul>
      <li>Uptime: 99.9% (43.2 min/month)</li>
      <li>API Success Rate: 99.5%</li>
      <li>Session Creation: 99%</li>
      <li>Share Operations: 99.9%</li>
    </ul>
  </Card>
  
  <Card>
    <CardHeader>
      <CardTitle>âš¡ Latency SLOs</CardTitle>
    </CardHeader>
    <ul>
      <li>P50 Response Time: &lt; 100ms</li>
      <li>P95 Response Time: &lt; 500ms</li>
      <li>P99 Response Time: &lt; 1000ms</li>
      <li>Blockchain Calls: &lt; 2s</li>
    </ul>
  </Card>
  
  <Card>
    <CardHeader>
      <CardTitle>ðŸ“ˆ Throughput Targets</CardTitle>
    </CardHeader>
    <ul>
      <li>Requests/sec: 100+</li>
      <li>Concurrent Sessions: 1000+</li>
      <li>Shares/hour: 10000+</li>
      <li>Cache Hit Rate: &gt; 80%</li>
    </ul>
  </Card>
</div>

### Performance Tuning

```bash
# Monitor real-time performance
htop

# Check network connections
ss -tuln | grep 8000

# Monitor disk I/O
iotop

# Check Redis performance
redis-cli --latency

# LevelDB statistics
curl http://localhost:8000/api/v1/admin/leveldb-stats
```

## Troubleshooting Guide

### Common Issues

<Callout type="error" title="High Memory Usage">
  **Symptoms:** Memory usage > 80%, OOM errors
  
  **Diagnosis:**
  ```bash
  # Check memory usage
  free -h
  ps aux | grep neuralock | awk '{sum+=$6} END {print sum/1024 " MB"}'
  
  # Check Redis memory
  redis-cli info memory
  ```
  
  **Solutions:**
  1. Increase Redis maxmemory limit
  2. Enable LevelDB compression
  3. Reduce cache sizes
  4. Add memory to server
</Callout>

<Callout type="warning" title="Slow API Response">
  **Symptoms:** P95 latency > 1s, timeouts
  
  **Diagnosis:**
  ```bash
  # Check slow queries
  grep "duration_ms" /opt/neuralock/logs/neuralock.log | \
    awk '$2 > 1000' | tail -20
  
  # Profile endpoints
  curl -w "@curl-format.txt" -o /dev/null -s \
    http://localhost:8000/api/v1/health
  ```
  
  **Solutions:**
  1. Check blockchain RPC latency
  2. Verify Redis connection pooling
  3. Enable query caching
  4. Optimize database indexes
</Callout>

### Debug Tools

```bash
# Enable debug logging
export NEURALOCK_LOG_LEVEL=DEBUG
systemctl restart neuralock

# Trace specific request
curl -H "X-Trace-ID: debug-123" http://localhost:8000/api/v1/info

# Database inspection
python scripts/inspect_db.py --path /data/leveldb

# Memory profiling
py-spy record -o profile.svg --pid $(pgrep -f neuralock)
```

## Monitoring Best Practices

<div style={{ display: 'grid', gridTemplateColumns: 'repeat(auto-fit, minmax(300px, 1fr))', gap: '1rem' }}>
  <Card>
    <CardHeader>
      <CardTitle>ðŸ“‹ Setup Checklist</CardTitle>
    </CardHeader>
    <ul>
      <li>Configure Prometheus scraping</li>
      <li>Import Grafana dashboards</li>
      <li>Setup critical alerts</li>
      <li>Configure log aggregation</li>
      <li>Test alert notifications</li>
      <li>Document runbooks</li>
    </ul>
  </Card>
  
  <Card>
    <CardHeader>
      <CardTitle>ðŸ”„ Regular Tasks</CardTitle>
    </CardHeader>
    <ul>
      <li>Review metrics weekly</li>
      <li>Update alert thresholds</li>
      <li>Analyze error trends</li>
      <li>Capacity planning</li>
      <li>Performance optimization</li>
      <li>Security audit logs</li>
    </ul>
  </Card>
  
  <Card>
    <CardHeader>
      <CardTitle>ðŸš¨ Incident Response</CardTitle>
    </CardHeader>
    <ul>
      <li>Alert acknowledgment SLA</li>
      <li>Escalation procedures</li>
      <li>Post-mortem process</li>
      <li>Root cause analysis</li>
      <li>Preventive measures</li>
      <li>Knowledge sharing</li>
    </ul>
  </Card>
</div>

## Next Steps

- Configure monitoring for your deployment
- Set up alerting based on your SLOs
- Create custom dashboards for your use case
- Implement automated remediation scripts