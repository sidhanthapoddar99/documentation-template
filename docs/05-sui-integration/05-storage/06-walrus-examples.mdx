---
id: walrus-examples
title: Walrus Use Cases & Examples
sidebar_label: Examples
sidebar_position: 5
---

import { Card, CardHeader, CardTitle, CardDescription } from '@site/src/components/Card';
import {CollapsibleCodeBlock} from '@site/src/components/CodeBlock';

# <img src="/img/icons/code.svg" width="32" height="32" style={{ verticalAlign: 'middle', marginRight: '12px' }} />Walrus Use Cases & Examples

Real-world implementation examples using both SDK and HTTP API approaches for common NeuraLabs scenarios.

## Overview

This guide provides practical examples of integrating Walrus storage into the NeuraLabs platform, demonstrating both TypeScript SDK and HTTP API implementations for each use case.

<div className="card padding--md mt-4">
  <h3><img src="/img/icons/info.svg" width="20" height="20" style={{ verticalAlign: 'middle', marginRight: '8px' }} />Example Categories</h3>
  <ul>
    <li><strong>AI Model Storage:</strong> Store and retrieve large AI models and weights</li>
    <li><strong>Workflow Management:</strong> Save encrypted workflow configurations</li>
    <li><strong>Dataset Handling:</strong> Manage training and validation datasets</li>
    <li><strong>Result Archiving:</strong> Store computation results and logs</li>
    <li><strong>Asset Management:</strong> Handle multimedia and document storage</li>
  </ul>
</div>

## AI Model Storage

### Storing Large AI Models

<CollapsibleCodeBlock
  title="AI Model Storage - TypeScript SDK"
  description="Store AI models with metadata and chunking using the SDK"
  language="typescript"
  defaultCollapsed={false}
>
{`// ai-model-storage-sdk.ts
import { WalrusClient } from '@mysten/walrus';
import { SuiClient } from '@mysten/sui/client';
import { Ed25519Keypair } from '@mysten/sui/keypairs/ed25519';
import * as fs from 'fs/promises';
import * as path from 'path';

interface AIModel {
  name: string;
  version: string;
  architecture: string;
  framework: string;
  files: {
    config: string;
    weights: string;
    tokenizer?: string;
  };
}

export class AIModelStorage {
  private walrusClient: WalrusClient;
  private signer: Ed25519Keypair;
  
  constructor(
    suiClient: SuiClient,
    signer: Ed25519Keypair
  ) {
    this.walrusClient = new WalrusClient({
      network: 'testnet',
      suiClient
    });
    this.signer = signer;
  }
  
  async storeModel(model: AIModel): Promise<{
    manifestBlobId: string;
    configBlobId: string;
    weightsBlobId: string;
    tokenizerBlobId?: string;
  }> {
    console.log(\`Storing AI model: \${model.name} v\${model.version}\`);
    
    // 1. Store model configuration
    const configData = await fs.readFile(model.files.config);
    const configResult = await this.walrusClient.writeBlob({
      blob: new Uint8Array(configData),
      deletable: false,
      epochs: 100, // Long-term storage for models
      signer: this.signer
    });
    
    console.log(\`Config stored: \${configResult.blobId}\`);
    
    // 2. Store model weights (handle large files)
    const weightsData = await fs.readFile(model.files.weights);
    const weightsBlobId = await this.storeLargeFile(
      weightsData,
      \`\${model.name}-weights-v\${model.version}\`
    );
    
    console.log(\`Weights stored: \${weightsBlobId}\`);
    
    // 3. Store tokenizer if present
    let tokenizerBlobId: string | undefined;
    if (model.files.tokenizer) {
      const tokenizerData = await fs.readFile(model.files.tokenizer);
      const tokenizerResult = await this.walrusClient.writeBlob({
        blob: new Uint8Array(tokenizerData),
        deletable: false,
        epochs: 100,
        signer: this.signer
      });
      tokenizerBlobId = tokenizerResult.blobId;
      console.log(\`Tokenizer stored: \${tokenizerBlobId}\`);
    }
    
    // 4. Create and store manifest
    const manifest = {
      model: {
        name: model.name,
        version: model.version,
        architecture: model.architecture,
        framework: model.framework,
      },
      blobs: {
        config: configResult.blobId,
        weights: weightsBlobId,
        tokenizer: tokenizerBlobId,
      },
      metadata: {
        storedAt: new Date().toISOString(),
        storedBy: this.signer.toSuiAddress(),
        checksums: {
          config: await this.calculateChecksum(configData),
          weights: await this.calculateChecksum(weightsData),
        }
      }
    };
    
    const manifestData = new TextEncoder().encode(
      JSON.stringify(manifest, null, 2)
    );
    
    const manifestResult = await this.walrusClient.writeBlob({
      blob: manifestData,
      deletable: false,
      epochs: 100,
      signer: this.signer
    });
    
    console.log(\`Manifest stored: \${manifestResult.blobId}\`);
    
    return {
      manifestBlobId: manifestResult.blobId,
      configBlobId: configResult.blobId,
      weightsBlobId,
      tokenizerBlobId,
    };
  }
  
  async loadModel(manifestBlobId: string): Promise<{
    model: AIModel;
    config: Uint8Array;
    weights: Uint8Array;
    tokenizer?: Uint8Array;
  }> {
    // 1. Load manifest
    const manifestData = await this.walrusClient.readBlob({
      blobId: manifestBlobId
    });
    
    const manifest = JSON.parse(
      new TextDecoder().decode(manifestData)
    );
    
    console.log(\`Loading model: \${manifest.model.name} v\${manifest.model.version}\`);
    
    // 2. Load configuration
    const configData = await this.walrusClient.readBlob({
      blobId: manifest.blobs.config
    });
    
    // 3. Load weights (handle large files)
    const weightsData = await this.loadLargeFile(manifest.blobs.weights);
    
    // 4. Load tokenizer if present
    let tokenizerData: Uint8Array | undefined;
    if (manifest.blobs.tokenizer) {
      tokenizerData = await this.walrusClient.readBlob({
        blobId: manifest.blobs.tokenizer
      });
    }
    
    // 5. Verify checksums
    const configChecksum = await this.calculateChecksum(configData);
    const weightsChecksum = await this.calculateChecksum(weightsData);
    
    if (configChecksum !== manifest.metadata.checksums.config ||
        weightsChecksum !== manifest.metadata.checksums.weights) {
      throw new Error('Checksum verification failed');
    }
    
    return {
      model: manifest.model,
      config: configData,
      weights: weightsData,
      tokenizer: tokenizerData,
    };
  }
  
  private async storeLargeFile(
    data: Buffer,
    name: string
  ): Promise<string> {
    const chunkSize = 5 * 1024 * 1024; // 5MB chunks
    const chunks: string[] = [];
    
    // Split into chunks
    for (let i = 0; i < data.length; i += chunkSize) {
      const chunk = data.slice(i, i + chunkSize);
      const result = await this.walrusClient.writeBlob({
        blob: new Uint8Array(chunk),
        deletable: false,
        epochs: 100,
        signer: this.signer
      });
      chunks.push(result.blobId);
      
      console.log(\`Uploaded chunk \${chunks.length}: \${result.blobId}\`);
    }
    
    // Store chunk manifest
    const chunkManifest = {
      name,
      totalSize: data.length,
      chunkSize,
      chunks,
    };
    
    const manifestResult = await this.walrusClient.writeBlob({
      blob: new TextEncoder().encode(JSON.stringify(chunkManifest)),
      deletable: false,
      epochs: 100,
      signer: this.signer
    });
    
    return manifestResult.blobId;
  }
  
  private async loadLargeFile(manifestBlobId: string): Promise<Uint8Array> {
    const manifestData = await this.walrusClient.readBlob({
      blobId: manifestBlobId
    });
    
    const manifest = JSON.parse(new TextDecoder().decode(manifestData));
    const chunks: Uint8Array[] = [];
    
    // Load all chunks
    for (const chunkId of manifest.chunks) {
      const chunkData = await this.walrusClient.readBlob({
        blobId: chunkId
      });
      chunks.push(chunkData);
    }
    
    // Combine chunks
    const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
    const combined = new Uint8Array(totalLength);
    let offset = 0;
    
    for (const chunk of chunks) {
      combined.set(chunk, offset);
      offset += chunk.length;
    }
    
    return combined;
  }
  
  private async calculateChecksum(data: Uint8Array): Promise<string> {
    const hashBuffer = await crypto.subtle.digest('SHA-256', data);
    const hashArray = Array.from(new Uint8Array(hashBuffer));
    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
  }
}

// Usage example
async function storeGPTModel() {
  const suiClient = new SuiClient({
    url: 'https://fullnode.testnet.sui.io:443'
  });
  
  const signer = Ed25519Keypair.fromSecretKey(
    Buffer.from(process.env.PRIVATE_KEY!, 'hex')
  );
  
  const storage = new AIModelStorage(suiClient, signer);
  
  const result = await storage.storeModel({
    name: 'gpt-2-small',
    version: '1.0.0',
    architecture: 'transformer',
    framework: 'pytorch',
    files: {
      config: './models/gpt2/config.json',
      weights: './models/gpt2/pytorch_model.bin',
      tokenizer: './models/gpt2/tokenizer.json',
    }
  });
  
  console.log('Model stored successfully:', result);
  
  // Later, load the model
  const loaded = await storage.loadModel(result.manifestBlobId);
  console.log('Model loaded:', loaded.model);
}`}
</CollapsibleCodeBlock>

<CollapsibleCodeBlock
  title="AI Model Storage - HTTP API"
  description="Store AI models using direct HTTP API calls"
  language="python"
  defaultCollapsed={false}
>
{`# ai_model_storage_http.py
import aiohttp
import asyncio
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import aiofiles

class AIModelStorageHTTP:
    def __init__(
        self,
        publisher_url: str = "https://publisher.walrus-testnet.walrus.space",
        aggregator_url: str = "https://aggregator.walrus-testnet.walrus.space"
    ):
        self.publisher_url = publisher_url
        self.aggregator_url = aggregator_url
        self.chunk_size = 5 * 1024 * 1024  # 5MB chunks
    
    async def store_model(
        self,
        model_name: str,
        model_version: str,
        config_path: Path,
        weights_path: Path,
        tokenizer_path: Optional[Path] = None,
        epochs: int = 100
    ) -> Dict[str, str]:
        """Store an AI model with all its components."""
        
        print(f"Storing AI model: {model_name} v{model_version}")
        
        async with aiohttp.ClientSession() as session:
            # 1. Store configuration
            config_blob_id = await self._upload_file(
                session,
                config_path,
                epochs,
                {"type": "model_config", "model": model_name, "version": model_version}
            )
            print(f"Config stored: {config_blob_id}")
            
            # 2. Store weights (chunked for large files)
            weights_blob_id = await self._upload_large_file(
                session,
                weights_path,
                epochs,
                {"type": "model_weights", "model": model_name, "version": model_version}
            )
            print(f"Weights stored: {weights_blob_id}")
            
            # 3. Store tokenizer if present
            tokenizer_blob_id = None
            if tokenizer_path and tokenizer_path.exists():
                tokenizer_blob_id = await self._upload_file(
                    session,
                    tokenizer_path,
                    epochs,
                    {"type": "tokenizer", "model": model_name, "version": model_version}
                )
                print(f"Tokenizer stored: {tokenizer_blob_id}")
            
            # 4. Create and store manifest
            manifest = {
                "model": {
                    "name": model_name,
                    "version": model_version,
                    "framework": "pytorch",  # Could be parameterized
                },
                "blobs": {
                    "config": config_blob_id,
                    "weights": weights_blob_id,
                    "tokenizer": tokenizer_blob_id,
                },
                "metadata": {
                    "stored_at": datetime.utcnow().isoformat(),
                    "checksums": {
                        "config": await self._calculate_file_hash(config_path),
                        "weights": await self._calculate_file_hash(weights_path),
                    }
                }
            }
            
            manifest_blob_id = await self._upload_json(
                session,
                manifest,
                epochs,
                {"type": "model_manifest", "model": model_name, "version": model_version}
            )
            print(f"Manifest stored: {manifest_blob_id}")
            
            return {
                "manifest_blob_id": manifest_blob_id,
                "config_blob_id": config_blob_id,
                "weights_blob_id": weights_blob_id,
                "tokenizer_blob_id": tokenizer_blob_id,
            }
    
    async def load_model(
        self,
        manifest_blob_id: str,
        output_dir: Path
    ) -> Dict[str, Path]:
        """Load an AI model from Walrus."""
        
        async with aiohttp.ClientSession() as session:
            # 1. Load manifest
            manifest = await self._download_json(session, manifest_blob_id)
            model_name = manifest["model"]["name"]
            model_version = manifest["model"]["version"]
            
            print(f"Loading model: {model_name} v{model_version}")
            
            # Create output directory
            model_dir = output_dir / f"{model_name}_v{model_version}"
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # 2. Download configuration
            config_path = model_dir / "config.json"
            await self._download_file(
                session,
                manifest["blobs"]["config"],
                config_path
            )
            print(f"Config downloaded: {config_path}")
            
            # 3. Download weights
            weights_path = model_dir / "pytorch_model.bin"
            if await self._is_chunked_file(session, manifest["blobs"]["weights"]):
                await self._download_large_file(
                    session,
                    manifest["blobs"]["weights"],
                    weights_path
                )
            else:
                await self._download_file(
                    session,
                    manifest["blobs"]["weights"],
                    weights_path
                )
            print(f"Weights downloaded: {weights_path}")
            
            # 4. Download tokenizer if present
            tokenizer_path = None
            if manifest["blobs"].get("tokenizer"):
                tokenizer_path = model_dir / "tokenizer.json"
                await self._download_file(
                    session,
                    manifest["blobs"]["tokenizer"],
                    tokenizer_path
                )
                print(f"Tokenizer downloaded: {tokenizer_path}")
            
            # 5. Verify checksums
            config_hash = await self._calculate_file_hash(config_path)
            weights_hash = await self._calculate_file_hash(weights_path)
            
            if (config_hash != manifest["metadata"]["checksums"]["config"] or
                weights_hash != manifest["metadata"]["checksums"]["weights"]):
                raise ValueError("Checksum verification failed")
            
            print("Checksum verification passed")
            
            return {
                "model_dir": model_dir,
                "config": config_path,
                "weights": weights_path,
                "tokenizer": tokenizer_path,
            }
    
    async def _upload_file(
        self,
        session: aiohttp.ClientSession,
        file_path: Path,
        epochs: int,
        metadata: Dict
    ) -> str:
        """Upload a single file."""
        form = aiohttp.FormData()
        
        async with aiofiles.open(file_path, 'rb') as f:
            content = await f.read()
            form.add_field('file', content, filename=file_path.name)
        
        form.add_field('epochs', str(epochs))
        form.add_field('metadata', json.dumps(metadata))
        
        async with session.post(f"{self.publisher_url}/v1/store", data=form) as resp:
            if resp.status != 200:
                raise Exception(f"Upload failed: {await resp.text()}")
            
            result = await resp.json()
            return result.get('blob_id') or result.get('blobId')
    
    async def _upload_large_file(
        self,
        session: aiohttp.ClientSession,
        file_path: Path,
        epochs: int,
        metadata: Dict
    ) -> str:
        """Upload a large file in chunks."""
        file_size = file_path.stat().st_size
        chunk_ids = []
        
        async with aiofiles.open(file_path, 'rb') as f:
            chunk_num = 0
            while True:
                chunk = await f.read(self.chunk_size)
                if not chunk:
                    break
                
                chunk_metadata = {
                    **metadata,
                    "chunk": chunk_num,
                    "total_chunks": (file_size + self.chunk_size - 1) // self.chunk_size
                }
                
                form = aiohttp.FormData()
                form.add_field('file', chunk, filename=f"{file_path.name}.chunk{chunk_num}")
                form.add_field('epochs', str(epochs))
                form.add_field('metadata', json.dumps(chunk_metadata))
                
                async with session.post(f"{self.publisher_url}/v1/store", data=form) as resp:
                    if resp.status != 200:
                        raise Exception(f"Chunk upload failed: {await resp.text()}")
                    
                    result = await resp.json()
                    chunk_id = result.get('blob_id') or result.get('blobId')
                    chunk_ids.append(chunk_id)
                
                chunk_num += 1
                print(f"Uploaded chunk {chunk_num}/{(file_size + self.chunk_size - 1) // self.chunk_size}")
        
        # Upload chunk manifest
        chunk_manifest = {
            "filename": file_path.name,
            "total_size": file_size,
            "chunk_size": self.chunk_size,
            "chunks": chunk_ids,
            "metadata": metadata
        }
        
        return await self._upload_json(session, chunk_manifest, epochs, {
            **metadata,
            "type": "chunk_manifest"
        })
    
    async def _upload_json(
        self,
        session: aiohttp.ClientSession,
        data: Dict,
        epochs: int,
        metadata: Dict
    ) -> str:
        """Upload JSON data."""
        form = aiohttp.FormData()
        form.add_field('file', json.dumps(data, indent=2), filename="data.json")
        form.add_field('epochs', str(epochs))
        form.add_field('metadata', json.dumps(metadata))
        
        async with session.post(f"{self.publisher_url}/v1/store", data=form) as resp:
            if resp.status != 200:
                raise Exception(f"Upload failed: {await resp.text()}")
            
            result = await resp.json()
            return result.get('blob_id') or result.get('blobId')
    
    async def _download_file(
        self,
        session: aiohttp.ClientSession,
        blob_id: str,
        output_path: Path
    ):
        """Download a file from Walrus."""
        async with session.get(f"{self.aggregator_url}/v1/{blob_id}") as resp:
            if resp.status != 200:
                raise Exception(f"Download failed: {resp.status}")
            
            async with aiofiles.open(output_path, 'wb') as f:
                async for chunk in resp.content.iter_chunked(8192):
                    await f.write(chunk)
    
    async def _download_json(
        self,
        session: aiohttp.ClientSession,
        blob_id: str
    ) -> Dict:
        """Download and parse JSON from Walrus."""
        async with session.get(f"{self.aggregator_url}/v1/{blob_id}") as resp:
            if resp.status != 200:
                raise Exception(f"Download failed: {resp.status}")
            
            return await resp.json()
    
    async def _download_large_file(
        self,
        session: aiohttp.ClientSession,
        manifest_blob_id: str,
        output_path: Path
    ):
        """Download a large file from chunks."""
        # Get chunk manifest
        manifest = await self._download_json(session, manifest_blob_id)
        
        async with aiofiles.open(output_path, 'wb') as f:
            for i, chunk_id in enumerate(manifest["chunks"]):
                print(f"Downloading chunk {i+1}/{len(manifest['chunks'])}")
                
                async with session.get(f"{self.aggregator_url}/v1/{chunk_id}") as resp:
                    if resp.status != 200:
                        raise Exception(f"Chunk download failed: {resp.status}")
                    
                    async for data in resp.content.iter_chunked(8192):
                        await f.write(data)
    
    async def _is_chunked_file(
        self,
        session: aiohttp.ClientSession,
        blob_id: str
    ) -> bool:
        """Check if a blob is a chunk manifest."""
        async with session.head(f"{self.aggregator_url}/v1/{blob_id}") as resp:
            metadata = resp.headers.get('x-walrus-metadata')
            if metadata:
                meta = json.loads(metadata)
                return meta.get('type') == 'chunk_manifest'
        return False
    
    async def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate SHA256 hash of a file."""
        sha256_hash = hashlib.sha256()
        async with aiofiles.open(file_path, 'rb') as f:
            while chunk := await f.read(8192):
                sha256_hash.update(chunk)
        return sha256_hash.hexdigest()

# Usage example
async def main():
    storage = AIModelStorageHTTP()
    
    # Store model
    result = await storage.store_model(
        model_name="llama-2-7b",
        model_version="1.0.0",
        config_path=Path("./models/llama2/config.json"),
        weights_path=Path("./models/llama2/pytorch_model.bin"),
        tokenizer_path=Path("./models/llama2/tokenizer.json"),
        epochs=100
    )
    
    print("Model stored:", result)
    
    # Load model
    loaded = await storage.load_model(
        manifest_blob_id=result["manifest_blob_id"],
        output_dir=Path("./downloaded_models")
    )
    
    print("Model loaded:", loaded)

if __name__ == "__main__":
    import datetime
    asyncio.run(main())`}
</CollapsibleCodeBlock>

## Workflow Storage

### Encrypted Workflow Management

<CollapsibleCodeBlock
  title="Workflow Storage - TypeScript SDK"
  description="Store encrypted AI workflows with access control"
  language="typescript"
  defaultCollapsed={false}
>
{`// workflow-storage-sdk.ts
import { WalrusClient } from '@mysten/walrus';
import { SuiClient } from '@mysten/sui/client';
import { Ed25519Keypair } from '@mysten/sui/keypairs/ed25519';
import { SealClient } from './seal-client';

interface AIWorkflow {
  id: string;
  name: string;
  description: string;
  nodes: WorkflowNode[];
  edges: WorkflowEdge[];
  parameters: Record<string, any>;
  version: string;
}

interface WorkflowNode {
  id: string;
  type: string;
  data: any;
  position: { x: number; y: number };
}

interface WorkflowEdge {
  id: string;
  source: string;
  target: string;
  data?: any;
}

export class WorkflowStorage {
  private walrusClient: WalrusClient;
  private sealClient: SealClient;
  private signer: Ed25519Keypair;
  
  constructor(
    suiClient: SuiClient,
    signer: Ed25519Keypair
  ) {
    this.walrusClient = new WalrusClient({
      network: 'testnet',
      suiClient
    });
    this.sealClient = new SealClient(suiClient);
    this.signer = signer;
  }
  
  async saveWorkflow(
    workflow: AIWorkflow,
    nftId: string,
    options: {
      compress?: boolean;
      epochs?: number;
      collaborators?: string[];
    } = {}
  ): Promise<{
    workflowId: string;
    blobId: string;
    version: string;
    encryptionKey?: string;
  }> {
    try {
      console.log(\`Saving workflow: \${workflow.name}\`);
      
      // 1. Prepare workflow data
      const workflowData = {
        ...workflow,
        metadata: {
          savedAt: new Date().toISOString(),
          savedBy: this.signer.toSuiAddress(),
          nftId,
          collaborators: options.collaborators || [],
        }
      };
      
      // 2. Serialize workflow
      let serialized = new TextEncoder().encode(
        JSON.stringify(workflowData, null, 2)
      );
      
      // 3. Compress if requested
      if (options.compress) {
        serialized = await this.compressData(serialized);
        console.log(\`Compressed: \${serialized.length} bytes\`);
      }
      
      // 4. Encrypt with Seal
      const encryptionResult = await this.sealClient.encrypt(
        serialized,
        nftId,
        options.collaborators?.length || 1
      );
      
      // 5. Store encrypted data on Walrus
      const storeResult = await this.walrusClient.writeBlob({
        blob: encryptionResult.encryptedData,
        deletable: false,
        epochs: options.epochs || 30,
        signer: this.signer
      });
      
      console.log(\`Workflow stored: \${storeResult.blobId}\`);
      
      // 6. Store metadata on blockchain
      await this.storeWorkflowMetadata({
        workflowId: workflow.id,
        blobId: storeResult.blobId,
        version: workflow.version,
        nftId,
        compressed: options.compress || false,
      });
      
      return {
        workflowId: workflow.id,
        blobId: storeResult.blobId,
        version: workflow.version,
        encryptionKey: encryptionResult.backupKey,
      };
      
    } catch (error) {
      console.error('Failed to save workflow:', error);
      throw error;
    }
  }
  
  async loadWorkflow(
    workflowId: string,
    nftId: string
  ): Promise<AIWorkflow> {
    try {
      // 1. Get workflow metadata from blockchain
      const metadata = await this.getWorkflowMetadata(workflowId);
      
      if (!metadata) {
        throw new Error('Workflow not found');
      }
      
      console.log(\`Loading workflow: \${workflowId}\`);
      
      // 2. Retrieve encrypted data from Walrus
      const encryptedData = await this.walrusClient.readBlob({
        blobId: metadata.blobId
      });
      
      // 3. Decrypt with Seal
      const decryptedData = await this.sealClient.decrypt(
        encryptedData,
        nftId
      );
      
      // 4. Decompress if needed
      let workflowData = decryptedData;
      if (metadata.compressed) {
        workflowData = await this.decompressData(decryptedData);
      }
      
      // 5. Parse workflow
      const workflow = JSON.parse(
        new TextDecoder().decode(workflowData)
      );
      
      return workflow;
      
    } catch (error) {
      console.error('Failed to load workflow:', error);
      throw error;
    }
  }
  
  async listWorkflows(
    ownerAddress?: string
  ): Promise<Array<{
    workflowId: string;
    name: string;
    version: string;
    savedAt: string;
    blobId: string;
  }>> {
    // Query blockchain for workflow metadata
    // Implementation depends on your smart contract
    const workflows = await this.queryWorkflows(ownerAddress);
    
    return workflows.map(w => ({
      workflowId: w.id,
      name: w.name,
      version: w.version,
      savedAt: w.savedAt,
      blobId: w.blobId,
    }));
  }
  
  async shareWorkflow(
    workflowId: string,
    recipientAddress: string,
    permissions: string[]
  ): Promise<boolean> {
    try {
      // Update access control on blockchain
      await this.updateWorkflowAccess({
        workflowId,
        recipientAddress,
        permissions,
      });
      
      console.log(\`Workflow \${workflowId} shared with \${recipientAddress}\`);
      return true;
      
    } catch (error) {
      console.error('Failed to share workflow:', error);
      return false;
    }
  }
  
  async createWorkflowVersion(
    baseWorkflowId: string,
    updates: Partial<AIWorkflow>,
    nftId: string
  ): Promise<{
    workflowId: string;
    blobId: string;
    version: string;
  }> {
    // 1. Load current workflow
    const currentWorkflow = await this.loadWorkflow(baseWorkflowId, nftId);
    
    // 2. Apply updates
    const newWorkflow: AIWorkflow = {
      ...currentWorkflow,
      ...updates,
      id: \`\${baseWorkflowId}-v\${Date.now()}\`,
      version: this.incrementVersion(currentWorkflow.version),
    };
    
    // 3. Save new version
    const result = await this.saveWorkflow(newWorkflow, nftId, {
      compress: true,
      epochs: 50,
    });
    
    // 4. Link versions on blockchain
    await this.linkWorkflowVersions({
      parentId: baseWorkflowId,
      childId: newWorkflow.id,
      version: newWorkflow.version,
    });
    
    return result;
  }
  
  private async compressData(data: Uint8Array): Promise<Uint8Array> {
    const compressionStream = new CompressionStream('gzip');
    const writer = compressionStream.writable.getWriter();
    writer.write(data);
    writer.close();
    
    const chunks: Uint8Array[] = [];
    const reader = compressionStream.readable.getReader();
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      chunks.push(value);
    }
    
    const compressed = new Uint8Array(
      chunks.reduce((acc, chunk) => acc + chunk.length, 0)
    );
    let offset = 0;
    for (const chunk of chunks) {
      compressed.set(chunk, offset);
      offset += chunk.length;
    }
    
    return compressed;
  }
  
  private async decompressData(data: Uint8Array): Promise<Uint8Array> {
    const decompressionStream = new DecompressionStream('gzip');
    const writer = decompressionStream.writable.getWriter();
    writer.write(data);
    writer.close();
    
    const chunks: Uint8Array[] = [];
    const reader = decompressionStream.readable.getReader();
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      chunks.push(value);
    }
    
    const decompressed = new Uint8Array(
      chunks.reduce((acc, chunk) => acc + chunk.length, 0)
    );
    let offset = 0;
    for (const chunk of chunks) {
      decompressed.set(chunk, offset);
      offset += chunk.length;
    }
    
    return decompressed;
  }
  
  private incrementVersion(version: string): string {
    const parts = version.split('.');
    const patch = parseInt(parts[2] || '0') + 1;
    return \`\${parts[0]}.\${parts[1]}.\${patch}\`;
  }
  
  // Placeholder methods for blockchain interaction
  private async storeWorkflowMetadata(metadata: any): Promise<void> {
    console.log('Storing metadata on blockchain:', metadata);
  }
  
  private async getWorkflowMetadata(workflowId: string): Promise<any> {
    console.log('Getting metadata from blockchain:', workflowId);
    return { blobId: 'mock-blob-id', compressed: true };
  }
  
  private async queryWorkflows(ownerAddress?: string): Promise<any[]> {
    console.log('Querying workflows:', ownerAddress);
    return [];
  }
  
  private async updateWorkflowAccess(params: any): Promise<void> {
    console.log('Updating workflow access:', params);
  }
  
  private async linkWorkflowVersions(params: any): Promise<void> {
    console.log('Linking workflow versions:', params);
  }
}

// Usage example
async function workflowExample() {
  const suiClient = new SuiClient({
    url: 'https://fullnode.testnet.sui.io:443'
  });
  
  const signer = Ed25519Keypair.fromSecretKey(
    Buffer.from(process.env.PRIVATE_KEY!, 'hex')
  );
  
  const storage = new WorkflowStorage(suiClient, signer);
  
  // Create a sample workflow
  const workflow: AIWorkflow = {
    id: 'workflow-001',
    name: 'Image Classification Pipeline',
    description: 'AI workflow for image classification',
    version: '1.0.0',
    nodes: [
      {
        id: 'input-1',
        type: 'input',
        data: { label: 'Image Input' },
        position: { x: 100, y: 100 }
      },
      {
        id: 'preprocess-1',
        type: 'preprocessing',
        data: { 
          operations: ['resize', 'normalize'],
          targetSize: [224, 224]
        },
        position: { x: 300, y: 100 }
      },
      {
        id: 'model-1',
        type: 'ai_model',
        data: {
          modelId: 'resnet50',
          modelBlobId: 'model-blob-123'
        },
        position: { x: 500, y: 100 }
      },
      {
        id: 'output-1',
        type: 'output',
        data: { format: 'json' },
        position: { x: 700, y: 100 }
      }
    ],
    edges: [
      { id: 'e1', source: 'input-1', target: 'preprocess-1' },
      { id: 'e2', source: 'preprocess-1', target: 'model-1' },
      { id: 'e3', source: 'model-1', target: 'output-1' }
    ],
    parameters: {
      batchSize: 32,
      confidence_threshold: 0.8
    }
  };
  
  // Save workflow
  const saveResult = await storage.saveWorkflow(
    workflow,
    'nft-123456', // User's NFT ID
    {
      compress: true,
      epochs: 50,
      collaborators: ['0xabc...', '0xdef...']
    }
  );
  
  console.log('Workflow saved:', saveResult);
  
  // Load workflow
  const loadedWorkflow = await storage.loadWorkflow(
    saveResult.workflowId,
    'nft-123456'
  );
  
  console.log('Workflow loaded:', loadedWorkflow.name);
}`}
</CollapsibleCodeBlock>

<CollapsibleCodeBlock
  title="Workflow Storage - HTTP API"
  description="Store encrypted AI workflows using HTTP API"
  language="python"
  defaultCollapsed={true}
>
{`# workflow_storage_http.py
import aiohttp
import asyncio
import json
import gzip
from typing import Dict, List, Optional, Any
from datetime import datetime
from cryptography.fernet import Fernet
import base64

class WorkflowStorageHTTP:
    def __init__(
        self,
        publisher_url: str = "https://publisher.walrus-testnet.walrus.space",
        aggregator_url: str = "https://aggregator.walrus-testnet.walrus.space",
        encryption_key: Optional[str] = None
    ):
        self.publisher_url = publisher_url
        self.aggregator_url = aggregator_url
        
        # Initialize encryption
        if encryption_key:
            self.cipher = Fernet(encryption_key.encode())
        else:
            # Generate new key
            key = Fernet.generate_key()
            self.cipher = Fernet(key)
            print(f"Generated encryption key: {key.decode()}")
    
    async def save_workflow(
        self,
        workflow: Dict[str, Any],
        nft_id: str,
        compress: bool = True,
        epochs: int = 30
    ) -> Dict[str, str]:
        """Save an encrypted workflow to Walrus."""
        
        print(f"Saving workflow: {workflow['name']}")
        
        # Add metadata
        workflow_data = {
            **workflow,
            "metadata": {
                "saved_at": datetime.utcnow().isoformat(),
                "nft_id": nft_id,
                "compressed": compress,
            }
        }
        
        # Serialize
        serialized = json.dumps(workflow_data, indent=2).encode()
        
        # Compress if requested
        if compress:
            serialized = gzip.compress(serialized)
            print(f"Compressed to {len(serialized)} bytes")
        
        # Encrypt
        encrypted = self.cipher.encrypt(serialized)
        print(f"Encrypted to {len(encrypted)} bytes")
        
        # Upload to Walrus
        async with aiohttp.ClientSession() as session:
            form = aiohttp.FormData()
            form.add_field(
                'file',
                encrypted,
                filename=f"{workflow['id']}.encrypted"
            )
            form.add_field('epochs', str(epochs))
            form.add_field('metadata', json.dumps({
                "type": "encrypted_workflow",
                "workflow_id": workflow['id'],
                "workflow_name": workflow['name'],
                "version": workflow.get('version', '1.0.0'),
                "compressed": compress,
                "nft_id": nft_id
            }))
            
            async with session.post(
                f"{self.publisher_url}/v1/store",
                data=form
            ) as resp:
                if resp.status != 200:
                    raise Exception(f"Upload failed: {await resp.text()}")
                
                result = await resp.json()
                blob_id = result.get('blob_id') or result.get('blobId')
        
        print(f"Workflow stored: {blob_id}")
        
        return {
            "workflow_id": workflow['id'],
            "blob_id": blob_id,
            "version": workflow.get('version', '1.0.0'),
            "encryption_key": self.cipher._signing_key.decode()
        }
    
    async def load_workflow(
        self,
        blob_id: str,
        encryption_key: Optional[str] = None
    ) -> Dict[str, Any]:
        """Load and decrypt a workflow from Walrus."""
        
        print(f"Loading workflow from: {blob_id}")
        
        # Use provided key or default
        if encryption_key:
            cipher = Fernet(encryption_key.encode())
        else:
            cipher = self.cipher
        
        async with aiohttp.ClientSession() as session:
            # Download encrypted data
            async with session.get(
                f"{self.aggregator_url}/v1/{blob_id}"
            ) as resp:
                if resp.status != 200:
                    raise Exception(f"Download failed: {resp.status}")
                
                encrypted_data = await resp.read()
                
                # Get metadata
                metadata_header = resp.headers.get('x-walrus-metadata')
                metadata = json.loads(metadata_header) if metadata_header else {}
        
        # Decrypt
        decrypted = cipher.decrypt(encrypted_data)
        print(f"Decrypted {len(decrypted)} bytes")
        
        # Decompress if needed
        if metadata.get('compressed', False):
            decrypted = gzip.decompress(decrypted)
            print(f"Decompressed to {len(decrypted)} bytes")
        
        # Parse workflow
        workflow = json.loads(decrypted.decode())
        
        return workflow
    
    async def list_workflows(
        self,
        filter_params: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, Any]]:
        """List available workflows (would query blockchain in real impl)."""
        # In a real implementation, this would query the blockchain
        # for workflow metadata based on NFT ownership
        
        # Mock implementation
        return [
            {
                "workflow_id": "workflow-001",
                "name": "Image Classification",
                "version": "1.0.0",
                "blob_id": "mock-blob-id-1",
                "saved_at": "2025-01-25T10:00:00Z"
            },
            {
                "workflow_id": "workflow-002",
                "name": "Text Generation",
                "version": "2.1.0",
                "blob_id": "mock-blob-id-2",
                "saved_at": "2025-01-24T15:30:00Z"
            }
        ]
    
    async def create_workflow_version(
        self,
        base_blob_id: str,
        updates: Dict[str, Any],
        nft_id: str,
        encryption_key: Optional[str] = None
    ) -> Dict[str, str]:
        """Create a new version of an existing workflow."""
        
        # Load current workflow
        current_workflow = await self.load_workflow(
            base_blob_id,
            encryption_key
        )
        
        # Apply updates
        new_workflow = {
            **current_workflow,
            **updates,
            "id": f"{current_workflow['id']}-v{int(datetime.now().timestamp())}",
            "version": self._increment_version(current_workflow.get('version', '1.0.0')),
            "parent_version": current_workflow.get('version', '1.0.0'),
            "parent_blob_id": base_blob_id
        }
        
        # Save new version
        return await self.save_workflow(
            new_workflow,
            nft_id,
            compress=True,
            epochs=50
        )
    
    async def export_workflow(
        self,
        blob_id: str,
        format: str = "json",
        encryption_key: Optional[str] = None
    ) -> bytes:
        """Export workflow in different formats."""
        
        workflow = await self.load_workflow(blob_id, encryption_key)
        
        if format == "json":
            return json.dumps(workflow, indent=2).encode()
        
        elif format == "yaml":
            import yaml
            return yaml.dump(workflow).encode()
        
        elif format == "python":
            # Generate Python code representation
            code = f"""# Generated workflow: {workflow['name']}
# Version: {workflow.get('version', '1.0.0')}

workflow = {repr(workflow)}

# Example usage:
# executor = WorkflowExecutor()
# result = executor.run(workflow)
"""
            return code.encode()
        
        else:
            raise ValueError(f"Unsupported format: {format}")
    
    def _increment_version(self, version: str) -> str:
        """Increment semantic version."""
        parts = version.split('.')
        patch = int(parts[2]) + 1
        return f"{parts[0]}.{parts[1]}.{patch}"

# Example workflow execution storage
class WorkflowExecutionStorage:
    def __init__(self, storage: WorkflowStorageHTTP):
        self.storage = storage
    
    async def save_execution_result(
        self,
        workflow_id: str,
        execution_id: str,
        results: Dict[str, Any],
        logs: List[str],
        epochs: int = 7  # Short-term storage for results
    ) -> str:
        """Save workflow execution results."""
        
        execution_data = {
            "workflow_id": workflow_id,
            "execution_id": execution_id,
            "timestamp": datetime.utcnow().isoformat(),
            "results": results,
            "logs": logs,
            "status": "completed",
            "metrics": {
                "execution_time": results.get('execution_time', 0),
                "tokens_used": results.get('tokens_used', 0),
                "cost": results.get('cost', 0)
            }
        }
        
        # Compress and upload
        serialized = json.dumps(execution_data).encode()
        compressed = gzip.compress(serialized)
        
        async with aiohttp.ClientSession() as session:
            form = aiohttp.FormData()
            form.add_field('file', compressed, filename=f"{execution_id}.json.gz")
            form.add_field('epochs', str(epochs))
            form.add_field('metadata', json.dumps({
                "type": "execution_result",
                "workflow_id": workflow_id,
                "execution_id": execution_id,
                "compressed": True
            }))
            
            async with session.post(
                f"{self.storage.publisher_url}/v1/store",
                data=form
            ) as resp:
                result = await resp.json()
                return result.get('blob_id') or result.get('blobId')

# Usage example
async def workflow_example():
    storage = WorkflowStorageHTTP()
    
    # Sample workflow
    workflow = {
        "id": "workflow-001",
        "name": "Sentiment Analysis Pipeline",
        "description": "Analyze sentiment from social media posts",
        "version": "1.0.0",
        "nodes": [
            {
                "id": "input",
                "type": "text_input",
                "config": {"source": "twitter"}
            },
            {
                "id": "preprocess",
                "type": "text_preprocessing",
                "config": {"operations": ["lowercase", "remove_urls", "tokenize"]}
            },
            {
                "id": "sentiment",
                "type": "ai_model",
                "config": {"model": "sentiment-bert", "threshold": 0.7}
            },
            {
                "id": "output",
                "type": "json_output",
                "config": {"format": "detailed"}
            }
        ],
        "edges": [
            {"source": "input", "target": "preprocess"},
            {"source": "preprocess", "target": "sentiment"},
            {"source": "sentiment", "target": "output"}
        ]
    }
    
    # Save workflow
    result = await storage.save_workflow(
        workflow,
        nft_id="nft-123456",
        compress=True,
        epochs=30
    )
    print("Saved:", result)
    
    # Load workflow
    loaded = await storage.load_workflow(
        result["blob_id"],
        result["encryption_key"]
    )
    print("Loaded:", loaded["name"])
    
    # Create new version
    new_version = await storage.create_workflow_version(
        result["blob_id"],
        {"description": "Updated sentiment analysis"},
        nft_id="nft-123456",
        encryption_key=result["encryption_key"]
    )
    print("New version:", new_version)

if __name__ == "__main__":
    asyncio.run(workflow_example())`}
</CollapsibleCodeBlock>

## Dataset Management

### Training Dataset Storage

<CollapsibleCodeBlock
  title="Dataset Storage - Combined Implementation"
  description="Store and manage large datasets for AI training"
  language="typescript"
  defaultCollapsed={false}
>
{`// dataset-storage.ts
import { WalrusClient } from '@mysten/walrus';
import axios from 'axios';
import * as fs from 'fs/promises';
import * as path from 'path';
import { createReadStream } from 'fs';
import FormData from 'form-data';

interface Dataset {
  id: string;
  name: string;
  description: string;
  type: 'image' | 'text' | 'audio' | 'tabular';
  format: string;
  size: number;
  samples: number;
  splits?: {
    train?: number;
    validation?: number;
    test?: number;
  };
}

export class DatasetStorage {
  private walrusClient?: WalrusClient;
  private httpClient = axios.create({
    timeout: 300000, // 5 minutes for large files
  });
  
  constructor(
    private config: {
      useSDK: boolean;
      publisherUrl?: string;
      aggregatorUrl?: string;
      walrusClient?: WalrusClient;
    }
  ) {
    if (config.useSDK && config.walrusClient) {
      this.walrusClient = config.walrusClient;
    }
  }
  
  async uploadDataset(
    datasetPath: string,
    metadata: Dataset,
    options: {
      chunkSize?: number;
      compress?: boolean;
      epochs?: number;
      onProgress?: (progress: number) => void;
    } = {}
  ): Promise<{
    datasetId: string;
    manifestBlobId: string;
    dataBlobIds: string[];
    totalSize: number;
  }> {
    console.log(\`Uploading dataset: \${metadata.name}\`);
    
    const stats = await fs.stat(datasetPath);
    const isDirectory = stats.isDirectory();
    
    if (isDirectory) {
      return this.uploadDatasetDirectory(datasetPath, metadata, options);
    } else {
      return this.uploadDatasetFile(datasetPath, metadata, options);
    }
  }
  
  private async uploadDatasetDirectory(
    dirPath: string,
    metadata: Dataset,
    options: any
  ): Promise<any> {
    const files = await this.getAllFiles(dirPath);
    const dataBlobIds: string[] = [];
    let totalUploaded = 0;
    const totalSize = await this.getTotalSize(files);
    
    // Group files by type
    const fileGroups = this.groupFilesByType(files, dirPath);
    
    for (const [groupName, groupFiles] of Object.entries(fileGroups)) {
      console.log(\`Uploading \${groupName}: \${groupFiles.length} files\`);
      
      for (const file of groupFiles) {
        const relativePath = path.relative(dirPath, file);
        const fileData = await fs.readFile(file);
        
        let blobId: string;
        if (this.config.useSDK && this.walrusClient) {
          // SDK approach
          const result = await this.walrusClient.writeBlob({
            blob: new Uint8Array(fileData),
            deletable: false,
            epochs: options.epochs || 50,
            signer: options.signer
          });
          blobId = result.blobId;
        } else {
          // HTTP API approach
          blobId = await this.uploadViaHttp(
            fileData,
            relativePath,
            options.epochs || 50
          );
        }
        
        dataBlobIds.push(blobId);
        totalUploaded += fileData.length;
        
        if (options.onProgress) {
          options.onProgress(totalUploaded / totalSize);
        }
      }
    }
    
    // Create and upload manifest
    const manifest = {
      dataset: metadata,
      structure: fileGroups,
      files: files.map((f, i) => ({
        path: path.relative(dirPath, f),
        blobId: dataBlobIds[i],
        size: (await fs.stat(f)).size
      })),
      uploadedAt: new Date().toISOString(),
      totalSize
    };
    
    const manifestBlobId = await this.uploadManifest(manifest, options);
    
    return {
      datasetId: metadata.id,
      manifestBlobId,
      dataBlobIds,
      totalSize
    };
  }
  
  private async uploadDatasetFile(
    filePath: string,
    metadata: Dataset,
    options: any
  ): Promise<any> {
    const fileSize = (await fs.stat(filePath)).size;
    const chunkSize = options.chunkSize || 50 * 1024 * 1024; // 50MB chunks
    
    if (fileSize <= chunkSize) {
      // Single file upload
      const fileData = await fs.readFile(filePath);
      
      let blobId: string;
      if (this.config.useSDK && this.walrusClient) {
        const result = await this.walrusClient.writeBlob({
          blob: new Uint8Array(fileData),
          deletable: false,
          epochs: options.epochs || 50,
          signer: options.signer
        });
        blobId = result.blobId;
      } else {
        blobId = await this.uploadViaHttp(
          fileData,
          path.basename(filePath),
          options.epochs || 50
        );
      }
      
      const manifest = {
        dataset: metadata,
        files: [{
          path: path.basename(filePath),
          blobId,
          size: fileSize
        }],
        uploadedAt: new Date().toISOString(),
        totalSize: fileSize
      };
      
      const manifestBlobId = await this.uploadManifest(manifest, options);
      
      return {
        datasetId: metadata.id,
        manifestBlobId,
        dataBlobIds: [blobId],
        totalSize: fileSize
      };
    } else {
      // Chunked upload for large files
      return this.uploadLargeDatasetFile(filePath, metadata, options);
    }
  }
  
  private async uploadLargeDatasetFile(
    filePath: string,
    metadata: Dataset,
    options: any
  ): Promise<any> {
    const fileSize = (await fs.stat(filePath)).size;
    const chunkSize = options.chunkSize || 50 * 1024 * 1024;
    const totalChunks = Math.ceil(fileSize / chunkSize);
    const chunkBlobIds: string[] = [];
    
    console.log(\`Uploading large file in \${totalChunks} chunks\`);
    
    const fileHandle = await fs.open(filePath, 'r');
    
    try {
      for (let i = 0; i < totalChunks; i++) {
        const buffer = Buffer.alloc(chunkSize);
        const { bytesRead } = await fileHandle.read(
          buffer,
          0,
          chunkSize,
          i * chunkSize
        );
        
        const chunkData = buffer.slice(0, bytesRead);
        
        let blobId: string;
        if (this.config.useSDK && this.walrusClient) {
          const result = await this.walrusClient.writeBlob({
            blob: new Uint8Array(chunkData),
            deletable: false,
            epochs: options.epochs || 50,
            signer: options.signer
          });
          blobId = result.blobId;
        } else {
          blobId = await this.uploadViaHttp(
            chunkData,
            \`\${path.basename(filePath)}.chunk\${i}\`,
            options.epochs || 50
          );
        }
        
        chunkBlobIds.push(blobId);
        
        if (options.onProgress) {
          options.onProgress((i + 1) / totalChunks);
        }
        
        console.log(\`Uploaded chunk \${i + 1}/\${totalChunks}: \${blobId}\`);
      }
    } finally {
      await fileHandle.close();
    }
    
    // Create chunk manifest
    const chunkManifest = {
      fileName: path.basename(filePath),
      fileSize,
      chunkSize,
      totalChunks,
      chunks: chunkBlobIds,
      checksum: await this.calculateFileChecksum(filePath)
    };
    
    const chunkManifestBlobId = await this.uploadManifest(
      chunkManifest,
      options
    );
    
    // Create dataset manifest
    const manifest = {
      dataset: metadata,
      files: [{
        path: path.basename(filePath),
        blobId: chunkManifestBlobId,
        size: fileSize,
        chunked: true
      }],
      uploadedAt: new Date().toISOString(),
      totalSize: fileSize
    };
    
    const manifestBlobId = await this.uploadManifest(manifest, options);
    
    return {
      datasetId: metadata.id,
      manifestBlobId,
      dataBlobIds: [chunkManifestBlobId, ...chunkBlobIds],
      totalSize: fileSize
    };
  }
  
  private async uploadViaHttp(
    data: Buffer,
    filename: string,
    epochs: number
  ): Promise<string> {
    const form = new FormData();
    form.append('file', data, filename);
    form.append('epochs', epochs.toString());
    
    const response = await this.httpClient.post(
      \`\${this.config.publisherUrl}/v1/store\`,
      form,
      {
        headers: form.getHeaders(),
        maxContentLength: Infinity,
        maxBodyLength: Infinity,
      }
    );
    
    return response.data.blob_id || response.data.blobId;
  }
  
  private async uploadManifest(
    manifest: any,
    options: any
  ): Promise<string> {
    const manifestData = new TextEncoder().encode(
      JSON.stringify(manifest, null, 2)
    );
    
    if (this.config.useSDK && this.walrusClient) {
      const result = await this.walrusClient.writeBlob({
        blob: manifestData,
        deletable: false,
        epochs: options.epochs || 50,
        signer: options.signer
      });
      return result.blobId;
    } else {
      return this.uploadViaHttp(
        Buffer.from(manifestData),
        'manifest.json',
        options.epochs || 50
      );
    }
  }
  
  async downloadDataset(
    manifestBlobId: string,
    outputDir: string,
    options: {
      onProgress?: (progress: number) => void;
    } = {}
  ): Promise<{
    dataset: Dataset;
    filesDownloaded: number;
    totalSize: number;
  }> {
    // Download manifest
    let manifestData: Uint8Array;
    
    if (this.config.useSDK && this.walrusClient) {
      manifestData = await this.walrusClient.readBlob({
        blobId: manifestBlobId
      });
    } else {
      const response = await this.httpClient.get(
        \`\${this.config.aggregatorUrl}/v1/\${manifestBlobId}\`,
        { responseType: 'arraybuffer' }
      );
      manifestData = new Uint8Array(response.data);
    }
    
    const manifest = JSON.parse(new TextDecoder().decode(manifestData));
    console.log(\`Downloading dataset: \${manifest.dataset.name}\`);
    
    // Create output directory
    await fs.mkdir(outputDir, { recursive: true });
    
    // Download files
    let downloadedSize = 0;
    let filesDownloaded = 0;
    
    for (const file of manifest.files) {
      const outputPath = path.join(outputDir, file.path);
      await fs.mkdir(path.dirname(outputPath), { recursive: true });
      
      if (file.chunked) {
        // Download chunked file
        await this.downloadChunkedFile(
          file.blobId,
          outputPath,
          (progress) => {
            const fileProgress = downloadedSize + (progress * file.size);
            options.onProgress?.(fileProgress / manifest.totalSize);
          }
        );
      } else {
        // Download regular file
        let fileData: Uint8Array;
        
        if (this.config.useSDK && this.walrusClient) {
          fileData = await this.walrusClient.readBlob({
            blobId: file.blobId
          });
        } else {
          const response = await this.httpClient.get(
            \`\${this.config.aggregatorUrl}/v1/\${file.blobId}\`,
            { responseType: 'arraybuffer' }
          );
          fileData = new Uint8Array(response.data);
        }
        
        await fs.writeFile(outputPath, fileData);
      }
      
      downloadedSize += file.size;
      filesDownloaded++;
      
      options.onProgress?.(downloadedSize / manifest.totalSize);
      console.log(\`Downloaded: \${file.path}\`);
    }
    
    return {
      dataset: manifest.dataset,
      filesDownloaded,
      totalSize: manifest.totalSize
    };
  }
  
  private async downloadChunkedFile(
    chunkManifestBlobId: string,
    outputPath: string,
    onProgress?: (progress: number) => void
  ): Promise<void> {
    // Download chunk manifest
    let manifestData: Uint8Array;
    
    if (this.config.useSDK && this.walrusClient) {
      manifestData = await this.walrusClient.readBlob({
        blobId: chunkManifestBlobId
      });
    } else {
      const response = await this.httpClient.get(
        \`\${this.config.aggregatorUrl}/v1/\${chunkManifestBlobId}\`,
        { responseType: 'arraybuffer' }
      );
      manifestData = new Uint8Array(response.data);
    }
    
    const chunkManifest = JSON.parse(new TextDecoder().decode(manifestData));
    
    // Download and write chunks
    const fileHandle = await fs.open(outputPath, 'w');
    
    try {
      for (let i = 0; i < chunkManifest.chunks.length; i++) {
        let chunkData: Uint8Array;
        
        if (this.config.useSDK && this.walrusClient) {
          chunkData = await this.walrusClient.readBlob({
            blobId: chunkManifest.chunks[i]
          });
        } else {
          const response = await this.httpClient.get(
            \`\${this.config.aggregatorUrl}/v1/\${chunkManifest.chunks[i]}\`,
            { responseType: 'arraybuffer' }
          );
          chunkData = new Uint8Array(response.data);
        }
        
        await fileHandle.write(chunkData, 0, chunkData.length, i * chunkManifest.chunkSize);
        
        onProgress?.((i + 1) / chunkManifest.chunks.length);
      }
    } finally {
      await fileHandle.close();
    }
    
    // Verify checksum
    const checksum = await this.calculateFileChecksum(outputPath);
    if (checksum !== chunkManifest.checksum) {
      throw new Error('Checksum verification failed');
    }
  }
  
  private async getAllFiles(dir: string): Promise<string[]> {
    const files: string[] = [];
    const entries = await fs.readdir(dir, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dir, entry.name);
      if (entry.isDirectory()) {
        files.push(...await this.getAllFiles(fullPath));
      } else {
        files.push(fullPath);
      }
    }
    
    return files;
  }
  
  private groupFilesByType(files: string[], baseDir: string): Record<string, string[]> {
    const groups: Record<string, string[]> = {};
    
    for (const file of files) {
      const relativePath = path.relative(baseDir, file);
      const parts = relativePath.split(path.sep);
      const groupName = parts[0] || 'root';
      
      if (!groups[groupName]) {
        groups[groupName] = [];
      }
      groups[groupName].push(file);
    }
    
    return groups;
  }
  
  private async getTotalSize(files: string[]): Promise<number> {
    let total = 0;
    for (const file of files) {
      const stats = await fs.stat(file);
      total += stats.size;
    }
    return total;
  }
  
  private async calculateFileChecksum(filePath: string): Promise<string> {
    const crypto = require('crypto');
    const hash = crypto.createHash('sha256');
    const stream = createReadStream(filePath);
    
    return new Promise((resolve, reject) => {
      stream.on('data', data => hash.update(data));
      stream.on('end', () => resolve(hash.digest('hex')));
      stream.on('error', reject);
    });
  }
}

// Usage example showing both SDK and HTTP approaches
async function datasetExample() {
  // Example 1: Using SDK
  const sdkStorage = new DatasetStorage({
    useSDK: true,
    walrusClient: new WalrusClient({
      network: 'testnet',
      suiClient: new SuiClient({ url: 'https://fullnode.testnet.sui.io:443' })
    })
  });
  
  // Example 2: Using HTTP API
  const httpStorage = new DatasetStorage({
    useSDK: false,
    publisherUrl: 'https://publisher.walrus-testnet.walrus.space',
    aggregatorUrl: 'https://aggregator.walrus-testnet.walrus.space'
  });
  
  const dataset: Dataset = {
    id: 'dataset-001',
    name: 'ImageNet Subset',
    description: 'Subset of ImageNet for testing',
    type: 'image',
    format: 'jpeg',
    size: 1024 * 1024 * 500, // 500MB
    samples: 1000,
    splits: {
      train: 800,
      validation: 100,
      test: 100
    }
  };
  
  // Upload dataset
  const uploadResult = await httpStorage.uploadDataset(
    './datasets/imagenet_subset',
    dataset,
    {
      epochs: 100,
      compress: false, // Images already compressed
      onProgress: (progress) => {
        console.log(\`Upload progress: \${(progress * 100).toFixed(2)}%\`);
      }
    }
  );
  
  console.log('Dataset uploaded:', uploadResult);
  
  // Download dataset
  const downloadResult = await httpStorage.downloadDataset(
    uploadResult.manifestBlobId,
    './downloaded_datasets/imagenet_subset',
    {
      onProgress: (progress) => {
        console.log(\`Download progress: \${(progress * 100).toFixed(2)}%\`);
      }
    }
  );
  
  console.log('Dataset downloaded:', downloadResult);
}`}
</CollapsibleCodeBlock>

## Result Storage

### Computation Result Archiving

<CollapsibleCodeBlock
  title="Result Storage Implementation"
  description="Store and retrieve AI computation results"
  language="typescript"
  defaultCollapsed={true}
>
{`// result-storage.ts
interface ComputationResult {
  jobId: string;
  workflowId: string;
  status: 'completed' | 'failed' | 'partial';
  startTime: Date;
  endTime: Date;
  outputs: {
    primary: any;
    intermediate?: Record<string, any>;
    logs: string[];
    metrics: Record<string, number>;
  };
  resources: {
    cpuTime: number;
    gpuTime?: number;
    memoryPeak: number;
    tokensUsed?: number;
  };
  cost: {
    compute: number;
    storage: number;
    total: number;
  };
}

export class ResultStorage {
  // Implementation for both SDK and HTTP
  async storeResult(
    result: ComputationResult,
    options: {
      compress?: boolean;
      epochs?: number;
      includeIntermediates?: boolean;
    } = {}
  ): Promise<{
    resultId: string;
    blobId: string;
    size: number;
  }> {
    // Store computation results with appropriate compression
    // and retention periods based on result type
    
    // Implementation details...
  }
  
  async retrieveResult(
    resultId: string
  ): Promise<ComputationResult> {
    // Retrieve and decompress results
    // Implementation details...
  }
  
  async queryResults(
    filters: {
      workflowId?: string;
      startDate?: Date;
      endDate?: Date;
      status?: string;
    }
  ): Promise<ComputationResult[]> {
    // Query results based on filters
    // Implementation details...
  }
}`}
</CollapsibleCodeBlock>

## Best Practices

<div className="grid grid-cols-1 md:grid-cols-2 gap-4 mt-4">
  <Card>
    <CardHeader>
      <CardTitle>
        <img src="/img/icons/success.svg" width="20" height="20" style={{ verticalAlign: 'middle', marginRight: '8px' }} />
        Storage Best Practices
      </CardTitle>
      <CardDescription>
         Always encrypt sensitive data<br/>
         Use compression for text/JSON<br/>
         Implement chunking for large files<br/>
         Store metadata separately<br/>
         Use appropriate epochs<br/>
         Verify data integrity
      </CardDescription>
    </CardHeader>
  </Card>

  <Card>
    <CardHeader>
      <CardTitle>
        <img src="/img/icons/terminal.svg" width="20" height="20" style={{ verticalAlign: 'middle', marginRight: '8px' }} />
        Performance Tips
      </CardTitle>
      <CardDescription>
         Batch small files together<br/>
         Use parallel uploads<br/>
         Implement caching layer<br/>
         Monitor storage costs<br/>
         Clean up expired data<br/>
         Use regional endpoints
      </CardDescription>
    </CardHeader>
  </Card>
</div>

## Monitoring and Analytics

<CollapsibleCodeBlock
  title="Storage Analytics Dashboard"
  description="Monitor Walrus storage usage and performance"
  language="typescript"
  defaultCollapsed={true}
>
{`// storage-analytics.ts
export class StorageAnalytics {
  async getUsageMetrics(): Promise<{
    totalStorage: number;
    activeBlobs: number;
    monthlyUploads: number;
    monthlyDownloads: number;
    costBreakdown: {
      storage: number;
      bandwidth: number;
      transactions: number;
    };
  }> {
    // Collect and aggregate storage metrics
    // Implementation details...
  }
  
  async generateReport(
    startDate: Date,
    endDate: Date
  ): Promise<AnalyticsReport> {
    // Generate detailed usage report
    // Implementation details...
  }
}`}
</CollapsibleCodeBlock>

## Next Steps

- Review the [Architecture Overview](/docs/sui-integration/storage/walrus-architecture) for system design
- Explore the [TypeScript SDK Guide](/docs/sui-integration/storage/walrus-sdk) for SDK details
- Try the [HTTP API Guide](/docs/sui-integration/storage/walrus-http-api) for REST integration
- Use the [CLI Guide](/docs/sui-integration/storage/walrus-cli) for command-line operations